var documenterSearchIndex = {"docs":
[{"location":"motifs/#Motifs:-shapes-and-sampling","page":"Motifs: shapes and sampling","title":"Motifs: shapes and sampling","text":"The library supports different motif shapes and sampling modes. Here, we present a brief explanation of how these mechanisms work and how you can create your own shape or sampling mode.","category":"section"},{"location":"motifs/#Shapes","page":"Motifs: shapes and sampling","title":"Shapes","text":"By default, the library includes five predefined motif shapes: :square, :triangle, :pair, :line, and :diagonal. These shapes are defined in the file src/rma/index.jl, and they determine how a motif is drawn by the library and how it is converted to a decimal value used as an index. Therefore, when constructing a motif shape, it is important to consider how it will be converted into a decimal value.\n\nFor example, a square motif can be converted into a decimal value using the following equation (without spatial generalization).\n\nI = sum_r = 0^n - 1sum_c = 0^n - 1 2^rn+cmathbfR_i+r j+c\n\nwhere 2^rn+c is responsible for associating each position of the motif with a power of 2, converting the binary structure into a decimal value. A Julia's function to compute it can be written as:\n\nfunction compute_index_square(x::AbstractArray, y::AbstractArray, parameters, structure::AbstractVector{Int}, func::F, dim::AbstractVector{Int}, fixed::Vector{Int}, itr::Vector{Int}, metric) where {F}\n\n    ##      Let a variable to store the index.\n    I = 0\n\n    ##      Copy the values of the fixed indeces to the vector of iterative indeces.\n    copy!(itr, fixed)       ##  We do it to avoid memory allocations =D\n\n    ##      Iterate to compute the index.\n    for r in 0:(structure[1] - 1)\n        for c in 0:(structure[2] - 1)\n            ##      Change the iterator.\n            itr[1] = fixed[1] + r\n            itr[2] = fixed[2] + c\n\n            ##      Calculate the recurrence between two positions.\n            if @inline func(x, y, parameters, itr, metric, dim)\n                index += 2^((r * structure[1]) + c)\n            end\n        end\n    end\n\n    return I + 1 ##     It is necessary for Julia indexing! i = I + 1\nend\n\nKnowing an algebraic expression to convert a motif into a decimal value is not strictly necessary, but it is recommended â€” especially considering the importance of understanding how this process will work for any value of n (if applicable). For example, consider a motif with an X-shape (for n = 3):\n\nbeginpmatrix\nxi_1            xi_2    \n         xi_3            \nxi_4            xi_5\nendpmatrix\n\nIt is easy to convert this shape into an index using something like:\n\nfunction compute_index_x(x::AbstractArray, y::AbstractArray, parameters, func::F, dim::AbstractVector{Int}, fixed::Vector{Int}, itr::Vector{Int}, metric) where {F}\n\n    ##      Let a variable to store the index.\n    I = 0\n\n    ##      Copy the values of the fixed indeces to the vector of iterative indeces.\n    copy!(itr, fixed)       ##  We do it to avoid memory allocations =D\n\n    ##  1. \\xi_1\n    if @inline func(x, y, parameters, itr, metric, dim)\n        I += 1\n    end\n\n    ##  2. \\xi_2\n    itr[2] = fixed[2] + 2\n    if @inline func(x, y, parameters, itr, metric, dim)\n        I += 2\n    end\n\n    ##  3. \\xi_3\n    itr[1] = fixed[1] + 1\n    itr[2] = fixed[2] + 1\n    if @inline func(x, y, parameters, itr, metric, dim)\n        I += 4\n    end\n\n    ##  4. \\xi_4\n    itr[1] = fixed[1] + 2\n    itr[2] = fixed[2]\n    if @inline func(x, y, parameters, itr, metric, dim)\n        I += 8\n    end\n\n    ##  5. \\xi_5\n    itr[2] = fixed[2] + 2\n    if @inline func(x, y, parameters, itr, metric, dim)\n        I += 16\n    end\n\n    return I + 1 ##     It is necessary for Julia indexing! i = I + 1\nend\n\nBut it is also clear that this code is not scalable, as it relies on a fixed structure. Transforming this into a scalable version that can handle any motif structure is one of the key computational challenges in the process of defining a new motif shape.\n\nThe application of a shape is associated with a sampling mode, so it is necessary to create a sampling function for each defined shape. This is because the sampling mode determines how the fixed parameter is set.","category":"section"},{"location":"motifs/#Sampling","page":"Motifs: shapes and sampling","title":"Sampling","text":"The sampling defines how each motif will be extracted from an RP, without computing the full RP explicitly. The library includes five predefined sampling modes: :full, :random, :triangleup, :columnwise and :columnwise_full. These modes are defined in the folder src/rma/histograms/, being called by the function distribution(...).\n\nThe difficulty of creating a sampling function is proportional to the complexity of your problem. For example, if you are sampling motifs randomly from the RP, the sampling function is simple. However, if the motifs need to be extracted from a specific structure, the difficulty increases.\n\nThe sampling code for a :random sampling using square motifs is\n\nfunction vect_square_random(x::AbstractArray, y::AbstractArray, parameters, structure::AbstractVector{Int},\n    space_size::AbstractVector{Int}, func::F, dim::AbstractVector{Int}, hv::Int, samples::Int, metric) where {F}\n\n    ##\n    ##      Alloc memory for the histogram and the index list.\n    hg = zeros(Int, 2^hv)   ##  Our histogram!\n    fixed = ones(Int, length(space_size))\n    itr = zeros(Int, length(space_size))\n\n    ##\n    ##      Compute the power vector.\n    p_vect = zeros(Int, hv)\n    for i in 1:hv\n        p_vect[i] = 2^(i - 1)\n    end\n\n    ##\n    ##      Get the samples and compute the histogram.\n    @inbounds for _ in 1:samples\n        ##\n        ##      Take a random index.\n        for s in eachindex(space_size)\n            fixed[s] = rand(1:space_size[s])\n        end\n\n        ##\n        ##      Compute the index and register the motif.\n        p = @fastmath compute_index_square(x, y, parameters, structure, func, dim, fixed, itr, p_vect, metric)\n        hg[p] += 1\n    end\n\n    ##\n    ##      Return the histogram.\n    return hg\nend\n\nIn this code, we compute the power vector (p_vect) before calling the compute_index_square function, so it is not necessary to calculate powers of 2 inside the loop. Moreover, we allocate memory for the iterator itr and the fixed indices fixed outside the loop to avoid unnecessary calls to the garbage collector. The for...\n\nfor s in eachindex(space_size)\n    fixed[s] = rand(1:space_size[s])\nend\n\n... retrieves an index set (ij) to define the first recurrence R_ij, , from which a motif is constructed using the shape function compute_index_square.","category":"section"},{"location":"distributions/#Recurrence-Motifs-Probability-Distributions","page":"Distributions","title":"Recurrence Motifs Probability Distributions","text":"RecurrenceMicrostatesAnalysis.jl aims to be a user-friendly library with powerful capabilities. It can be used through simple function calls or more advanced configurations that offer greater control. We will begin with the simpler usage, explaining its arguments and settings, and gradually move toward more complex configurations throughout this discussion.","category":"section"},{"location":"distributions/#One-dimensional-data","page":"Distributions","title":"One-dimensional data","text":"This section presents a run similar to the one shown on the quick start page, but with a more detailed explanation. For one-dimensional problems, such as the logistic map or the generalized Bernoulli shift map (Beta-X), you can use a vector of positions along the trajectory as input. To illustrate this, let's consider a uniform distribution:\n\nusing Distributions\ndata = rand(Uniform(0, 1), 3000)\n\nComputing the recurrence motif distribution is straightforward once the threshold and n (motif size) parameters are defined. A good value for threshold can be estimated using the find_parameters function, which we recommend using in most cases.\n\nusing RecurrenceMicrostatesAnalysis\nth, s = find_parameters(data, 3)\ndist = distribution(data, th, 3)\n\nwarning: Warning\nWe do not recommend the use of find_parameters inside a loop, as it needs to compute several distributions to find the threshold value that maximizes recurrence entropy, which can significantly reduce the library's performance. For this reason, we have not created an overload of the distribution function that automatically calculates the threshold. Instead, we suggest using an average threshold value computed from a few representative snippets of your dataset using the find_parameters function.\n\nThe distribution function includes several keyword arguments for configuration. Before moving on to the next section, we will discuss these arguments, as they apply to every call of the distribution function.","category":"section"},{"location":"distributions/#Motif-constrained-shape","page":"Distributions","title":"Motif constrained shape","text":"There are variations in motif constraint shapes proposed in the literature, such as the triangular motif. Supporting these shape generalizations is one of the goals of RecurrenceMicrostatesAnalysis.jl, and it is also a computational challenge. Adapting the conversion of motifs with a generic shape from a binary structure to a decimal value can be a very complex problem, and to support this in the library, we need to adapt the pipeline that converts a motif for each specific shape.\n\nCurrently, RecurrenceMicrostatesAnalysis.jl supports five shapes: square, triangle, diagonal, line, and pair. The way the library converts these motifs constrained shapes to decimal values is detailed on the motifs page. You can change the shape using the kword shape, which can be set to :square, :triangle, :diagonal, :line, or :pair. By default, the library uses :square as the default shape.\n\ndist = distribution(data, th, 3; shape = :triangle)\n\nnote: Note\nThe shape :pair doesn't require a value of n, since it always uses n=2. However, it is still necessary to informe a value to this parameter, that will be interpreted as the separation between two points in a diagonal.dist = distribution(data, th, 6; shape = :pair)When workign with shape :pair, we recommend you to use the full structure of distribution function.structure = [3, 9]\ndist = distribution(data, data, th, structure; shape = :pair)Here, structure defines the position of the second element based on the random position of the first element.","category":"section"},{"location":"distributions/#Motifs-sampling","page":"Distributions","title":"Motifs sampling","text":"The sampling mode defines how RecurrenceMicrostatesAnalysis.jl selects motifs from a recurrence space. Currently, the library supports four sampling modes: full, random, columnwise and triangle up. You can learn more about them on the motifs page, where we discuss how each mode works. The sampling mode can be configured using the keyword argument sampling_mode, which can be set to :full, :random, :columnwise, :columnwise_full, or :triangleup. By default, the library uses :random as the default sampling mode.\n\ndist = distribution(data, th, 3; sampling_mode = :full)\n\ncompat: Compat\nNot all sampling modes are compatible with certain motif constrained shapes, and the following table illustrates the compatibility between them. :full :random :columnwise :columnwise_full :triangleup\n:square checkmark checkmark checkmark checkmark checkmark\n:triangle checkmark checkmark   \n:diagonal  checkmark   \n:time  checkmark   \n:pair  checkmark checkmark  ","category":"section"},{"location":"distributions/#Run-mode","page":"Distributions","title":"Run mode","text":"RecurrenceMicrostatesAnalysis.jl has two run modes that results in a different output type. The run mode :vect allocates all required memory in beginning of the process, and return the distribution as a vector. This is the default configuration of the library for n  6.\n\ndist = distribution(data, th, 4; run_mode = :vect)\n\nThe run mode :dict uses dictionaries to allocate memory just when needed. The total allocation of dictionary mode can be greater than when using vectors, but the real memory allocation is smaller.\n\ndist = distribution(data, th, 4; run_mode = :dict)\n\ncompat: Compat\nIt is important to note that the shapes :diagonal, :line, and :pair are not compatible with run mode :dict. Additionally, sampling modes :columnwise and :columnwise_full return a matrix in which each column represents a probability distribution for a specif time value.dist = distribution(data, th, 2; sampling_mode = :columnwise)\ndist = distribution(data, th, 2; sampling_mode = :columnwise_full)","category":"section"},{"location":"distributions/#Number-of-samples","page":"Distributions","title":"Number of samples","text":"With exception of sampling modes :full and :columnwise_full, all sampling modes take motifs randomly in a recurrence space. The kword num_samples defines the number of samples that will be used by the library, it can be either an integer value that specifies the exact number, or a decimal value interpreted as the percentage of samples taken from the entire available population. By default, RecurrenceMicrostatesAnalysis.jl uses 5.\n\ndist = distribution(data, th, 3; num_samples = 0.1)\n\ndist = distribution(data, th, 3; num_samples = 50000)","category":"section"},{"location":"distributions/#Threads","page":"Distributions","title":"Threads","text":"RecurrenceMicrostatesAnalysis.jl is highly compatible with CPU asynchronous jobs, that can increase significantly the computational performance of the library. The kword threads defines if the library will use threads or not, being true by default. The number of threads used is equal to the number of threads available to Julia, being it configured by the environment variable JULIA_NUM_THREADS, or by the running argument --threads T in Julia initiation: For example, using julia --threads 8.\n\nusing BenchmarkTools\n@benchmark distribution(data, th, 4; sampling_mode = :full, threads = false)\n@benchmark distribution(data, th, 4; sampling_mode = :full, threads = true)\n\nwarning: Warning\nRecurrenceMicrostatesAnalysis.jl allocates memory for each thread, so how many threads you use, more memory the library will allocate. It is done to increase the performance, and avoid the memory concurrency.","category":"section"},{"location":"distributions/#Metrics","page":"Distributions","title":"Metrics","text":"RecurrenceMicrostatesAnalysis.jl uses the library Distances.jl to simplify the configuration of metrics, and increase the computation performance. With it, modify the metric is a easy process that can be done with the kword metric.\n\nusing Distances\nmy_metric = KLDivergence()\ndist = distribution(data, th, 2; metric = my_metric)\n\nwarning: Warning\nThe default recurrence functions were configured to metrics with two arguments, like euclidean(x, y), so if you need to use another type of metric, it is needed to define a new recurrence function, see Recurrence functions page to know more about it.","category":"section"},{"location":"distributions/#Recurrence-functions","page":"Distributions","title":"Recurrence functions","text":"A recurrence function defines if two points of a trajectory recurr or not. Actually the library have two recurrence functions available\n\nStandard recurrence: R(mathbfx mathbfy)=Theta(varepsilon - mathbfx-mathbfy)\nRecurrence with corridor threshold: R(mathbfx mathbfy)=Theta(mathbfx-mathbfy - varepsilon_min) cdot Theta(varepsilon_max - mathbfx-mathbfy)\n\nRecurrenceMicrostatesAnalysis.jl automatically change between them with the type of parameters, so if you use as parameter a Float64, the library will apply the standard recurrence, or, if you use a Tuple{Float64, Float64}, the library will apply the recurrence with corridor threshold.\n\ndist = (distribution(data, th, 2))'\ndist = (distribution(data, (0.0, th), 2))'\n\nIt is possible to write your own recurrece function, we talk more about it in the Recurrence functions page.","category":"section"},{"location":"distributions/#High-dimensionality-data","page":"Distributions","title":"High-dimensionality data","text":"If you are working with a dynamical system or a data time serie with two or more dimensions, it is important to note that RecurrenceMicrostatesAnalysis.jl effectively not works with vectors, but matrices. In this situation, each row of the matrix will represent a coordinate, and each column a set of coordinates along a trajectory. For example, if we want a uniform distribution with three dimension and 3,000 points, we will have something like:\n\nusing Distributions\ndata = rand(Uniform(0, 1), 3, 3000)\n\nThis format of data is effectvely what the library uses. In the case of previous section, when we are working with vectors, RecurrenceMicrostatesAnalysis.jl converts it to a matrix 1times 3000 but when we are working which data with a dimensionality different than one, it is necessary to use the proper format.\n\nusing RecurrenceMicrostatesAnalysis\nth, s = find_parameters(data, 3)\ndist = distribution(data, th, 3)","category":"section"},{"location":"distributions/#Continuous-problems","page":"Distributions","title":"Continuous problems","text":"Continuous problems means numerically integrate a differential equation problem and take the values as input to RecurrenceMicrostatesAnalysis.jl. Thinking in it, we make the library compatible with a powerful tool to solve these problems in Julia: the library DifferentialEquations.jl. The way to apply this kind of data in the library is similar with the other two cases discussed before, as we will demonstrate in this section. \n\ninfo: Info\nThe code of Lorenz system used in these examples was get from Example 2 of DifferentialEquations.jl documentation\n\nfunction lorenz!(du, u, p, t)\n    du[1] = 10.0 * (u[2] - u[1])\n    du[2] = u[1] * (28.0 - u[3]) - u[2]\n    du[3] = u[1] * u[2] - (8 / 3) * u[3]\nend\n\nusing DifferentialEquations\nu0 = [1.0; 0.0; 0.0]\ntspan = (0.0, 1000.0)\nprob = ODEProblem(lorenz!, u0, tspan)\nsol = solve(prob)\n\nWith the data computed, it is easy to apply to RecurrenceMicrostatesAnalysis.jl, with a simply memory access given by DifferentialEquations.jl.\n\nusing RecurrenceMicrostatesAnalysis\ndata = sol[:, :]\nth, s = find_parameters(data, 3)\ndist = distribution(data, th, 3)\n\nwarning: Warning\nAlthough it is possible to compute the distribution as demonstrated above, we strongly advise against doing so in this way.\n\nWe recommend you to apply a transient into your data and take a correct time resolution while doing the process of discretization, it is needed to maximize the information available. RecurrenceMicrostatesAnalysis.jl has a utilitary function to help with this process.\n\nprepared_data = prepare(sol, 0.2; transient = 10000, K = 1000)\nth, s = find_parameters(prepared_data, 3)\ndist = distribution(prepared_data, th, 3)\n\nIf you have the threshold parameter, it is also possible to simplify the call using:\n\ndist = distribution(sol, th, 3, 0.2; transient = 10000, K = 1000)","category":"section"},{"location":"distributions/#Spatial-data","page":"Distributions","title":"Spatial data","text":"RecurrenceMicrostatesAnalysis.jl is compatible with generalised recurrence plot analysis for spatial data proposed by Marwan, Kurths and Saparin at 2006 [9]. It allow the library to calculate a probability distribution of motifs in a tensorial recurrence space, for example, to images the recurrence space have four dimensions.\n\ntodo: Todo\nSince this is an open research field, the library is designed to support exploration and estimation for research purposes. We donâ€™t recommend applying it in production environments ðŸ˜‰\n\nThe application of RecurrenceMicrostatesAnalysis.jl to spatial data is very similar to the others presented before, but the input format is more complex. Instead to matrices we need to use abstract arrays with dimension D, where the first dimension will be interpreted as a coordinate dimension (such as for high-dimensionaly data), and rest of the dimensions will be the spatial data dimensionality. To illustrate it, let an image with RGB. It can be represented as an abstract array with 3 dimensions, where the first dimension will have a length 3, being each element a color value (red, blue and green), and the others two dimensions are relative to each pixel that compose the image. We will demonstrate it using a uniform distribution, where each position can be interpreted as a RGB pixel for an image 100x100.\n\nusing Distributions\ndata = rand(Uniform(0, 1), 3, 100, 100)\n\nWhen we work with spatial data is necessarity to use the complete structure of distribution function, defining a vector structure where each value represents the length of a motif constrained side. For example, to a square tensorial motif constrained with side 2, we can use:\n\nusing RecurrenceMicrostatesAnalysis\ndist = distribution(data, data, 0.5, [2, 2, 2, 2])\n\nSince the recurrence space has four dimensions, in this examples, it is necessary for structure has the same number of elements, where each element will represent the motif' side lenght for each dimension.\n\nwarning: Warning\nThe find_parameters function is not compatible with spatial data.\n\ncompat: Compat\nIt is important to note that this functionality is only available to motif shapes :square, :diagonal, :line and :pair, for :random sampling mode.","category":"section"},{"location":"performance/#Performance-tips-to-use-RecurrenceMicrostatesAnalysis.jl","page":"Performance Tips","title":"Performance tips to use RecurrenceMicrostatesAnalysis.jl","text":"One of the main goals of RecurrenceMicrostatesAnalysis.jl is its computational performance, being fast and light. For it, RecurrenceMicrostatesAnalysis.jl has a good memory managment, allocating only the necessary memory, and a good adaptability to multi-threading jobs, spliting the work between all available threads. For that, we recommend to always use threads = true with the distribution function, and define a number of threads different than one in the enverionment variable JULIA_NUM_THREADS, or openning julia using julia --threads 8.\n\nIt is crucial to note that how much larger is a dataset, more time is needed to the library compute a recurrence motif distribution, and the number of samples can also affect it. (Image: CPU Performance)\n\nWith respect of memory consumition, RecurrenceMicrostatesAnalysis.jl has even better performance, being extremally light. The library allocates only the necessary memory to store information, such as a vector with the number of each motif that there is in some recurrence space. It is possible to see in the following graphic the library memory usage when compared with standard approach.  (Image: RAM Performance)\n\nRecurrenceMicrostatesAnalysis.jl allocates memory for each thread, so when you increase the number of available threads, the library will allocate more memory to avoid concurrency. It is also necessary to allocate more memory when we increase the motif size n, that is based on the motif area sigma (our hypervolume for spatial generalization), so largest motifs needs more memory per thread.\n\nThese measures were made using the library BenchmarkTools.jl.\n\nusing Distributions, RecurrenceMicrostatesAnalysis, BenchmarkTools\ndata = rand(Uniform(0, 1), 10000);\n@benchmark distribution(data, 0.27, 3)\n@benchmark distribution(data, 0.27, 3; sampling_mode = :full)\n@benchmark distribution(data, 0.27, 4)\n@benchmark distribution(data, 0.27, 4; sampling_mode = :full)","category":"section"},{"location":"rqa/#Recurrence-Quantification-Analysis","page":"RQA","title":"Recurrence Quantification Analysis","text":"The recurrence microstate analysis allows us to estimate values of typical RQA measures, such as determinism and laminarity, with a good precision, and defines some novel quantifiers. We will demonstate in this page how compute these quantifiers using a uniform distribution as input.\n\nusing RecurrenceMicrostatesAnalysis, Distributions        # Generate our data =D\ndata = rand(Uniform(0, 1), 3000);\nth, s = find_parameters(data, 3)\ndist = distribution(data, th, 3);","category":"section"},{"location":"rqa/#Recurrence-Entropy","page":"RQA","title":"Recurrence Entropy","text":"To compute the recurrence entropy, it is possible to use the rentropy function that receives a recurrence motif probability distribution.\n\nentr = rentropy(dist)\n\ninfo: Info\nNote that the entropy computed is also returned by the find_parameters function.\n\nThe rentropy function has also a keyword argument that can be used to ignore some motifs.\n\nentr = rentropy(dist; ignore_motifs = [1, 512])\n\ninfo: Info\nNote that the kword ignore_motifs uses as index the notation of Julia, beginning in 1, instead 0. So, the motif 0 is identified by the number 1.","category":"section"},{"location":"rqa/#Recurrence-Rate","page":"RQA","title":"Recurrence Rate","text":"The recurrence rate (RR) can be computed using a similar method to the recurrence entropy.\n\nrr = rrate(dist)\n\nSince the recurrence rate is an estimated measure, it has a small error, how you can check in the following graphic, that displays the relative error between the RR computed by RecurrenceMicrostatesAnalysis.jl and the standard approach. (Image: Relative error of recurrence rate compared to the value computed using the standard method. Panel (f) provides an overview of the error distributions presented in panels (a)-(e).)","category":"section"},{"location":"rqa/#Determinism","page":"RQA","title":"Determinism","text":"The determinism (DET) can be computed using a recurrence motifs probability distribution and the recurrence rate. It is important to note that it can be done using two motif constrained shapes: :square or :diagonal.\n\ndet = determinism(rr, dist)\ndet = determinism(rr, distribution(data, th, 3; shape = :diagonal))\n\nSimilar to RR, the determinism is a quantifier estimated using recurrence microstates analysis, so it has a small error that is demonstrated in the following figure. (Image: Relative error of determinism compared to the value computed using the standard method.)\n\ninfo: Info\n(i) is the uniform distribution, (ii) is the Lorenz system, (iii) is the Logistic map, (iv) is the RÃ¶ssler system, and (v) is the Bernoulli shifted generalized.\n\nwarning: Warning\nDeterminism (DET) just can be computed using :square or :diagonal shapes.\n\nWe implement an way to do it without the need to compute the recurrence distributions.\n\ndet = determinism(data, th)\n\ninfo: Info\nWhen we estimate DET directly using this function overload, the library will automatically use a diagonal motif constrained shape.","category":"section"},{"location":"rqa/#Laminarity","page":"RQA","title":"Laminarity","text":"The laminarity (LAM) can be computed with a method similar to determinism (DET). It is important to note that it can be done using two motif constrained shapes: :square or :line.\n\nlam = laminarity(rr, dist)\nlam = laminarity(rr, distribution(data, th, 3; shape = :line))\n\nIn the same way, laminarity has a small error associated to it estimation. You can check it in the next figure. (Image: Relative error of laminarity compared to the value computed using the standard method.)\n\ninfo: Info\n(i) is the uniform distribution, (ii) is the Lorenz system, (iii) is the Logistic map, (iv) is the RÃ¶ssler system, and (v) is the Bernoulli shifted generalized.\n\nwarning: Warning\nLaminarity (LAM) just can be computed using :square or :line shapes.\n\nWe implement an way to do it without the need to compute the recurrence distributions.\n\nlam = laminarity(data, th)\n\ninfo: Info\nWhen we estimate LAM directly using this function overload, the library will automatically use a line motif constrained shape.","category":"section"},{"location":"rqa/#Disorder","page":"RQA","title":"Disorder","text":"Disorder is a very powerful quantifier introduced by Flauzino et al.[10] in 2025. The computation of this quantifier is very simple using RecurrenceMicrostatesAnalysis.jl:\n\nmicrostate_size = 3\nÎž = disorder(data, microstate_size)\n\ninfo: Info\nRead more about disorder here.\n\nwarning: Warning\nThe microstate size can only be 2, 3, or 4.\n\nwarning: Warning\nThe function disorder tries to maximize the disorder for a range of varepsilon. To compute it faster, we recommend defining a base value of varepsilon and a range around it. For example:Îž = disorder(data, microstate_size; Îµ_min = 0.9 * th, Îµ_max = 1.1 * th, Îµ_range_size = 6)","category":"section"},{"location":"bib/#Bibliography","page":"Bibliography","title":"Bibliography","text":"N.Â Marwan, M.Â C.Â Romano, M.Â Thiel and J.Â Kurths. Recurrence plots for the analysis of complex systems. PhysicsÂ Reports 438, 237â€“329 (2007).\n\n\n\nJ.-P.Â Eckmann, S.Â O.Â Kamphorst and D.Â Ruelle. Recurrence Plots of Dynamical Systems. EurophysicsÂ Letters 4, 973â€“977 (1987).\n\n\n\nC.Â L.Â Webber and N.Â Marwan. Recurrence Quantification Analysis: Theory and Best Practices (Springer, Cham, 2014).\n\n\n\nG.Â Corso, T.Â Lima Prado, G.Â Z.Â Santos Lima, J.Â Kurths and S.Â R.Â Lopes. Quantifying Entropy Using Recurrence Matrix Microstates. Chaos 28 (2018).\n\n\n\nY.Â Hirata. Recurrence plots for characterizing random dynamical systems. CommunicationsÂ inÂ NonlinearÂ ScienceÂ andÂ NumericalÂ Simulation 94 (2021).\n\n\n\nG.Â S.Â Spezzatto, J.Â V.Â Flauzino, G.Â Corso, B.Â R.Â Boaretto, E.Â E.Â Macau, T.Â Lima Prado and S.Â R.Â Lopes. Recurrence Microstates for Machine Learning Classification. Chaos 34, 073140 (2024).\n\n\n\nG.Â Marghoti, T.Â d.Â Prado, S.Â R.Â Lopes and Y.Â Hirata. Involution Symmetry Quantification Using Recurrences. PhysicalÂ ReviewÂ E 110 (2024).\n\n\n\nF.Â E.Â daÂ Cruz, T.Â d.Â Prado, S.Â R.Â Lopes, N.Â Marwan and J.Â Kurths. Density-based recurrence measures from microstates. PhysicalÂ ReviewÂ E 111 (2025).\n\n\n\nN.Â Marwan, J.Â Kurths and P.Â Saparin. Generalised Recurrence Plot Analysis for Spatial Data. PhysicsÂ LettersÂ A 360, 545â€“551 (2007).\n\n\n\nJ.Â V.Â Flauzino, T.Â d.Â Prado, N.Â Marwan, J.Â Kurths and S.Â R.Â Lopes. Quantifying Disorder in Data. Phys.Â Rev.Â Lett. 135 (2025).\n\n\n\nT.Â L.Â Prado, V.Â S.Â Machado, G.Â Corso, G.Â Z.Â Lima and S.Â R.Â Lopes. How to Compute Suitable Vicinity Parameter and Sampling Time of Recurrence Analysis. SSRN (2023).\n\n\n\nJ.Â S.Â Iwanski and E.Â Bradley. Recurrence Plots of Experimental Data: To Embed or Not to Embed? Chaos 8, 861â€“871 (1998).\n\n\n\n","category":"section"},{"location":"utils/#Utilitary-functions","page":"Utils","title":"Utilitary functions","text":"RecurrenceMicrostatesAnalysis.jl has several utilitary functions to help when using the library. These functions allows to simplify the process to implement recurrence analysis to a project, facilitating the set of a threshold, or the preparation of continuous data to be analysed.\n\nwarning: Warning\nSome function presented in this section was still in development, so these function can change in future versions.","category":"section"},{"location":"utils/#Preparing-a-continuous-data","page":"Utils","title":"Preparing a continuous data","text":"When working with continuous data, it is important to do a discretization, changing the time resolution to improve the available information. prepare is an utilitary function to discretize the data, applying a vicinity parameter to change the data time resolution [11].\n\nfunction lorenz!(du, u, p, t)\n    du[1] = 10.0 * (u[2] - u[1])\n    du[2] = u[1] * (28.0 - u[3]) - u[2]\n    du[3] = u[1] * u[2] - (8 / 3) * u[3]\nend\n\nusing DifferentialEquations, RecurrenceMicrostatesAnalysis\nu0 = [1.0; 0.0; 0.0];\ntspan = (0.0, 1000.0);\nprob = ODEProblem(lorenz!, u0, tspan);\nsol = solve(prob);\n\ndata_prepared = prepare(sol, 0.2)\n\ninfo: Info\nIt is possible to see the difference between the data solution and the prepareted data when we make a RP. (Image: RP without vicinity application and with vicinity)\n\nThe prepare function can also apply a transient phase to the data, using the kword transient, and define the data length, using the kword K.\n\ndata_prepared = prepare(sol, 0.2; transient = 6000, K = 1000)","category":"section"},{"location":"utils/#Finding-threshold","page":"Utils","title":"Finding threshold","text":"The threshold is a free parameter that need to be defined by the user when working with RP, RQA, or RMA. Using the principle of maximizing the recurrence entropy, we build a function to estimate a good value for threshold [11]. The find_parameters function returns a Float64 value to be used as threshold and the maximazed entropy. It is important to note that this function can be a little slower, since it needs to compute a recurrence motif distributions for each threshold in some interval.\n\nth, s = find_parameters(data_prepared, 3)","category":"section"},{"location":"api/#Public-API","page":"Public API","title":"Public API","text":"","category":"section"},{"location":"api/#Distribution","page":"Public API","title":"Distribution","text":"","category":"section"},{"location":"api/#RQA","page":"Public API","title":"RQA","text":"","category":"section"},{"location":"api/#Disorder","page":"Public API","title":"Disorder","text":"","category":"section"},{"location":"api/#Utilitary-Functions","page":"Public API","title":"Utilitary Functions","text":"","category":"section"},{"location":"api/#Recurrence-Functions","page":"Public API","title":"Recurrence Functions","text":"","category":"section"},{"location":"api/#Main.RecurrenceMicrostatesAnalysis.distribution","page":"Public API","title":"Main.RecurrenceMicrostatesAnalysis.distribution","text":"Based on Recurrence Plot\n\n    distribution([x], [parameters], n::Int; kwargs...)\n\nCompute the distribution of recurrence microstates probabilities from the dataset x. The input parameters consists of the constant values used to calculate the recurrence between two points. n is an integer that represents the length of motifs side.\n\nInput:\n\n[x]: input dataset.\n[parameter]: set of parameters used to compute the recurrence microstate distribution.\nn: microstate size.\n\nOutput: this function can return a vector, an array or a dictionary based on the number of possible microstates and the setting of run_mode or sampling_mode.\n\n\n\n\n\nBased on Cross-Recurrence Plot\n\ndistribution([x], [y], parameters, n::Int; kwords...)\n\nCompute the distribution of recurrence microstates probabilities from the datasets x and y. The input parameters consists of the constant values used to calculate the recurrence between two points. n is an integer that represents the length of motifs side.\n\nInput:\n\n[x]: input dataset.\n[y]: input dataset.\n[parameter]: set of parameters used to compute the recurrence microstate distribution.\nn: microstate size.\n\nOutput: this function can return a vector, an array or a dictionary based on the number of possible microstates and the setting of run_mode or sampling_mode.\n\n\n\n\n\nUsing DifferencialEquations.jl\n\ndistribution([solution], parameters, n::Int, vicinity::Union{Int, Float64}; kwords...)\n\nCompute the distribution of recurrence microstates probabilities from the solution of a differencial equation solved by  the library DifferencialEquations.jl. The input parameters consists of the constant values used to calculate the recurrence  between two points. n is an integer that represents the length of motifs side. vicinity is the time separation used to discretize a continuous problem.\n\nInput:\n\n[solution]: solution returned by the library DifferentialEquations.jl.\n[parameter]: set of parameters used to compute the recurrence microstate distribution.\nn: microstate size.\nÏƒ: sampling parameter; it defines the time resolution of discretized data.\n\nSpecific kwargs:\n\ntransient: defines an interval of time that will be ignored, and taked as a transient.\nK: defines the maximum size of the result time series.\n\nOutput: this function can return a vector, an array or a dictionary based on the number of possible microstates and the setting of run_mode or sampling_mode.\n\n\n\n\n\nMain\n\ndistribution([x], [y], parameters, [structure]; kwords...)\n\nCompute the distribution of recurrence microstates probabilities from the datasets x and y. The input parameters consists of the constant values used to calculate the recurrence between two points. Meanwhile, the input structure is a vector where each element represents a side of the motif.\n\nInput:\n\n[x]: input dataset.\n[y]: input dataset.\n[parameter]: set of parameters used to compute the recurrence microstate distribution.\n[structure]: microstate structure.\n\nkwargs:\n\nshape: microstate shape. Can be :square, :triangle, :pair, :diagonal or :line. (default :square)\nrun_mode: define the output format. It can be :vect for a Vector{Float64}, or :dict for a Dict{Int, Float64}. If you are you sampling_mode :columnwise`\nsampling_mode: define how the library will take motifs in a RP. Can be :full, :random, :triangleup, :columnwise or :columnwise_full. (default :random)\nnum_samples: number of samples used to compute the distribution. Can be an Int value or a Float64, which will be interpretad as a proportion of the total population of microstates in a RP. (This is not required for :full and :columnwise_full sampling modes)\nthreads: set if library will use asyncronous jobs or not.\nmetric: metric defined using the library Distances.jl.\nfunc: recurrence function.\n\nOutput: this function can return a vector, an array or a dictionary based on the number of possible microstates and the setting of run_mode or sampling_mode.\n\n\n\n\n\n","category":"function"},{"location":"api/#Main.RecurrenceMicrostatesAnalysis.rrate","page":"Public API","title":"Main.RecurrenceMicrostatesAnalysis.rrate","text":"rrate([probs])\n\nCompute the approximated recurrence rate of a RP from a probability distribution of recurrence microstates. Here, we use a relation between the mean recurrence rate of each motif and the desired value. It can be written as\n\nRR approx sum_I = 0^N mathbfp_I^(k)left(frac1k^2sum_i=1^ksum_j=1^k mathbfM_ij^(I)right)\n\nwhere mathbfM_ij^(I) is the motif structure.\n\nInput:\n\n[probs]: the vector of probabilities mathbfp^(k) computed using distribution(...).\n\nOutput: returns the recurrence rate as a Float64.\n\n\n\n\n\nrrate([x], [parameters], n::Int; shape::Symbol, sampling_mode::Symbol, r::Float64})\n\nCompute the approximated recurrence rate of a Recurrence Plot from a data [x] using a probability distribution of recurrence microstates computed from it.\n\nInput:\n\n[x]: input data.\n[parameter]: set of parameters used to compute the recurrence microstate distribution.\nn: microstate size.\nshape (kwarg): shape of the used motifs. :square by default, it can be: :square, :triangle, :pair, :diagonal, :line.\nsampling_mode (kwarg): sampling mode used. :random by default, it can be: :square, :triangle, :pair, :diagonal, :line.\nr (kwarg): ratio of the total number of microstates to be sampled for the histogram. (default r = 0.05)\n\nOutput: returns the recurrence rate as a Float64.\n\n\n\n\n\n","category":"function"},{"location":"api/#Main.RecurrenceMicrostatesAnalysis.rentropy","page":"Public API","title":"Main.RecurrenceMicrostatesAnalysis.rentropy","text":"rentropy([probs]; [ignore_motifs])\n\nCompute the recurrence entropy, as proposed by [4].\n\nInput:\n\n[probs]: a Vector{Float64} returned by the function distribution(...).\n[ignore_motifs] (kwarg): list of motifs to ignore.\n\nOutput: return the recurrence entropy as a Float64.\n\n\n\n\n\n","category":"function"},{"location":"api/#Main.RecurrenceMicrostatesAnalysis.determinism","page":"Public API","title":"Main.RecurrenceMicrostatesAnalysis.determinism","text":"determinism(rr::Float64, [probs])\n\nEstimate the determinism from a distribution. If the distribution has 512 elements, the function will consider square motifs, computing determinism using\n\nI^(beta) = frac1RR(2mathbfM_12^(beta) + 4mathbfM_13^(beta) + 8mathbfM_21^(beta) + 16 + 32mathbfM_23^(beta) + 64mathbfM_31^(beta) + 128mathbfM_32^(beta))\n\nwhere mathbfM is the motif structure. It defines a class of motifs (C_L) ni I^(beta) that we use to estimate DET:\n\nDET approx 1 - frac1RRsum_Iin (C_L) mathbfp^(3)_I\n\ndist = distribution(data, th, 3)\nrr = rrate(dist)\ndet = determinism(rr, dist)\n\nIf the distribution has 8 elements, this function will consider line motifs, which makes the process simpler. In this case, we just need the motif with I = 2:\n\nDET approx 1 - fracmathbfp^(3)_2RR\n\ndist = distribution(data, th, 3; shape = :line)\nrr = rrate(dist)\ndet = determinism(rr, dist)\n\nInput:\n\nrr: recurrence rate.\n[probs]: a Vector{Float64} returned by the function distribution(...).\n\nOutput: returns the determinism as a Float64.\n\n\n\n\n\ndeterminism([x], threshold::Float64; r::Float64)\n\nEstimate the determinism from a data [x]` using a probability distribution and a RR computed from it. \n\nInput:\n\n[x]: input data.\n[parameter]: set of parameters used to compute the recurrence microstate distribution.\nr (kwarg): ratio of the total number of microstates to be sampled for the histogram. (default r = 0.05)\n\nOutput: returns a Tuple{Float64, Float64}.\n\nlam: laminarity as Float64.\nrr: recurrence rate as Float64.\n\n\n\n\n\n","category":"function"},{"location":"api/#Main.RecurrenceMicrostatesAnalysis.laminarity","page":"Public API","title":"Main.RecurrenceMicrostatesAnalysis.laminarity","text":"laminarity(rr::Float64, [probs])\n\nEstimate the laminarity from a distribution. If the distribution has 512 elements, the function will consider square motifs, computing laminarity using\n\nI^(beta) = frac1RR(2 + 8mathbfM_21^(beta) + 16mathbfM_22^(beta) + 32mathbfM_23^(beta) + 64mathbfM_31^(beta) + 128mathbfM_32^(beta) + 256mathbfM_33^(beta))\n\nwhere mathbfM is the motif structure. It defines a class of motifs (C_L) ni I^(beta) that we use to estimate LAM:\n\nLAM approx 1 - frac1RRsum_Iin (C_L) mathbfp^(3)_I\n\ndist = distribution(data, th, 3)\nrr = rrate(dist)\nlam = laminarity(rr, dist)\n\nIf the distribution has 8 elements, this function will consider line motifs, which makes the process simpler. In this case, we just need the motif with I = 2:\n\nLAM approx 1 - fracmathbfp^(3)_2RR\n\ndist = distribution(data, th, 3; shape = :line)\nrr = rrate(dist)\nlam = laminarity(rr, dist)\n\nInput:\n\nrr: recurrence rate.\n[probs]: a Vector{Float64} returned by the function distribution(...).\n\nOutput: returns the laminarity as a Float64.\n\n\n\n\n\nlaminarity([x], threshold::Float64; r::Float64)\n\nEstimate the laminarity from a data [x]` using a probability distribution and a RR computed from it. \n\nInput:\n\n[x]: input data.\n[parameter]: set of parameters used to compute the recurrence microstate distribution.\nr (kwarg): ratio of the total number of microstates to be sampled for the histogram. (default r = 0.05)\n\nOutput: returns a Tuple{Float64, Float64}.\n\nlam: laminarity as Float64.\nrr: recurrence rate as Float64.\n\n\n\n\n\n","category":"function"},{"location":"api/#Main.RecurrenceMicrostatesAnalysis.disorder","page":"Public API","title":"Main.RecurrenceMicrostatesAnalysis.disorder","text":"disorder([x], n::Int; kwords...)\n\nCompute disorder of a time series [x], based on the work of Flauzino et al. (2025)[10](read here]).\n\nInput:\n\n[x]: input dataset.\nn: microstates' size, being n âˆˆ {2, 3, 4}.\n\nkwargs:\n\nÎµ: recurrence threshold used as reference to define the range of thresholds where we look for the maximum disorder.\nÎµ_min: minimum value of Îµ of the range.\nÎµ_max: maximum value of Îµ of the range.\nÎµ_range_size: number of elements in the range.\n\nOutput: return the disorder value as a Float64.\n\n\n\n\n\n","category":"function"},{"location":"api/#Main.RecurrenceMicrostatesAnalysis.Disorder.get_memory","page":"Public API","title":"Main.RecurrenceMicrostatesAnalysis.Disorder.get_memory","text":"get_memory(label::Vector{Vector{Int}})\n\nAllocate the necessary memory to compute disorder. Return a Vector{Float64}.\n\n\n\n\n\n","category":"function"},{"location":"api/#Main.RecurrenceMicrostatesAnalysis.Disorder.get_class_probs!","page":"Public API","title":"Main.RecurrenceMicrostatesAnalysis.Disorder.get_class_probs!","text":"get_class_probs!(memory::Vector{Float64}, probs::Vector{Float64}, label::Vector{Vector{Int}}, class::Int)\n\nGet the probabilities associated to a specific motif' class for a given motif size n. memory is a block of memory allocate by the function get_memory(n) that will be modified to store the probabilities from probs associated to the class.\n\n\n\n\n\n","category":"function"},{"location":"api/#Main.RecurrenceMicrostatesAnalysis.Disorder.get_norm_class_probs!","page":"Public API","title":"Main.RecurrenceMicrostatesAnalysis.Disorder.get_norm_class_probs!","text":"get_norm_class_probs!(memory::Vector{Float64}, probs::Vector{Float64}, label::Vector{Vector{Int}}, class::Int)\n\nGet the normalized probabilities associated to a specific motif' class for a given motif size n. memory is a block of memory allocate by the function get_memory(n) that will be modified to store the probabilities from probs associated to the class.\n\n\n\n\n\n","category":"function"},{"location":"api/#Main.RecurrenceMicrostatesAnalysis.prepare","page":"Public API","title":"Main.RecurrenceMicrostatesAnalysis.prepare","text":"prepare([solution], Ïƒ::Union{Float64, Int}; transient::Int, K::Int)\n\nPrepare a problem solved by the library DifferentialEquations.jl to be used in RecurrenceMicrostatesAnalysis.jl.  This function applies the sampling parameter (Ïƒ) to discretize the continuous time series, as proposed by [11].\n\nInput:\n\n[solution]: solution returned by the library DifferentialEquations.jl.\nÏƒ: sampling parameter; it defines the time resolution of discretized data.\ntransient (kwarg): number of points, without application of sampling, that will be ignored.\nK (kwarg): maximum length of the returned data series.\n\nOutput:\n\ndata: returns the prepared data in the format of a Matrix{Float64}. Each row represents a system component, and each column represents a time step.\n\n\n\n\n\n","category":"function"},{"location":"api/#Main.RecurrenceMicrostatesAnalysis.find_parameters","page":"Public API","title":"Main.RecurrenceMicrostatesAnalysis.find_parameters","text":"find_parameters([x], n::Int; r::Float64 = 0.05, Îµ_max_range = 0.5)\n\nThis function calculates the maximum microstate entropy for the RP of the input time series given the microstate size n and the input ratio r of the total number of microstates.\n\nInput:\n\n[x]: input data.\nn: microstate size.\nr (kwarg): ratio of the total number of microstates to be sampled for the histogram (default r = 0.05)\nÎµ_max_range (kwarg): percentage of the maximum distance to be used as the range in the process. (default Îµ_max_range = 0.5)\nfraction (kwarg): iteration fraction. (default fraction = 5)\nshape (kwarg): motif shape. (default shape = :square)\n\nOutput: (is a Tuple{Float64, Float64})\n\nÎµopt: the value of the vicinity parameter that maximizes the recurrence microstates entropy.\nSmax: maximum recurrence microstates entropy for the input time series.\n\n\n\n\n\n","category":"function"},{"location":"api/#Main.RecurrenceMicrostatesAnalysis.recurrence","page":"Public API","title":"Main.RecurrenceMicrostatesAnalysis.recurrence","text":"recurrence([x], [y], parameters, idx::AbstractVector{Int}, metric::Metric, dim::AbstractVector{Int})\n\nCompute the recurrence between two position defined by idx of the datasets x and y. parameters defines which type of recurrence it will use, like the standard recurrence and the recurrence with corridor threshold. metric defines the norm applied to the datasets, it uses the library Distances.jl to compute the metric. dim is just used for high-dimensional problems, such as images. A recurrence function is a function of the form\n\nR_ij = Theta(varepsilon - mathbfx_i - mathbfy_j)\n\nwhere Theta is the Heaviside function, cdot denotes an appropriate norm, and varepsilon is  a threshold parameter that defines the maximum distance between two points for them to be considered varepsilon-recurrent to each other.\n\n#       Examples\n#   For a 2D recurrence space.\n@inline function recurrence(...)\n    return @inbounds evaluate(metric, x[idx[1]], y[idx[2]]) <= threshold\nend\n\n#   For a recurrence tensor space (generalization to spatial data)\n@inline function recurrence(...)\n    return @inbounds evaluate(metric, view(x, :, view(idx, 1:dim[1])), view(y, :, view(idx, dim[1]+1:dim[1] + dim[2]))) <= threshold\nend\n\nInput:\n\n[x]: a dataset.\n[y]: a dataset.\n[parameter]: set of parameters used to compute the recurrence microstate distribution, i.e., the value of varepsilon.\n[idx]: vector of indeces from [x] and [y] to calculate the recurrence between them.\nmetric: metric from Distances.jl used to compute the recurrence. It defines the norm cdot.\n[dim]: number of dimensions derived from [x] and [y]. If you are using a time series it is usually [1,1].\n\nOutput: Recurrence functions return true when we have a recurrence, and false otherwise.\n\n\n\n\n\n","category":"function"},{"location":"theory/#A-Brief-Theoretical-Review","page":"Theoretical Overview","title":"A Brief Theoretical Review","text":"We do here a brief review about the recurrence field. If you are new in this area, we sincerelly expects that this review helps you ðŸ˜„. If you want a more formal review, please read the publication \"Recurrence plots for the analysis of complex systems\" [1].","category":"section"},{"location":"theory/#Recurrence-Plots","page":"Theoretical Overview","title":"Recurrence Plots","text":"Recurrence Plots (RPs) were introduced by Eckmann et al. at 1987 [2]. Given a trajectory mathbfx_i in mathbbR^m, with iin12K and K being the length of the analyzed time series, we define the RP as a graphical representation  based on the recurrence matrix define by mathbfR_ij = Theta(varepsilon - mathbfx_i - mathbfx_j) where cdot denotes an appropriate norm, and varepsilon is a threshold parameter that defines the maximum distance between two points for them to be considered varepsilon-recurrent to each other.\n\nIn Julia, it is possible to build a RP using the library RecurrenceAnalysis.jl (documentation here). For example:\n\nusing Distributions, RecurrenceAnalysis, CairoMakie\n\ndata = rand(Uniform(0, 1), 1000)\nr_matrix = RecurrenceMatrix(data, 0.27)\nfig, ax, rp = heatmap(r_matrix, colormap = :binary)\n\n(Image: RP of a uniform distribution.)\n\nBy definition, we have for a RP that mathbfR_ii = 1, which the main diagonal of a recurrence matrix is filled with recurrences, being called as line of identity (LOI). The structures presented in a RP can have crucial tips about a system, in a way that some system can be caracterized by its topology and texture [2, 3].\n\nTalking about RQA and RMA, we just have interest about the RP texture, that is the base for these two analysis metodologies. Since we do not will really formalize this topic, it is recommended to you read the original paper, available here.","category":"section"},{"location":"theory/#Recurrence-Quantification-Analysis","page":"Theoretical Overview","title":"Recurrence Quantification Analysis","text":"Recurrence Quantification Analysis (RQA) has become a standard tool for extracting nonlinear characteristics from time series. It relies on specific recurrence structures within Recurrence Plots (RPs), such as diagonal line lengths that are typically associated with deterministic dynamics. There are several quantification measures defined in the literature, and we strongly recommend you to read the book \"Recurrence Quantification Analysis, Theory and Best Practices\" [3]. In this review we just will talk about recurrence rate, determinism, laminarity and recurrence entropy, since they are the quantifiers that can be computed using RMA.\n\nThe recurrence rate (RR) is the overall density of points within a RP, and calculated as\n\nRR = frac1K^2-Ksum_i=1^Ksum_ineq j = 1^K mathbfR_ij\n\nIt can be used to help calculating dynamical invariants such as correlation sum and correlation dimension. These invariants are importat dynamical properties of a system, and offer meaningful evidence in the search for chaotic behavior.\n\nThe determinism (DET) also can be seem as a density, similarly to the RR in this regard. However, it is a percentage of points that are part of diagonal lines rather than just the total density. Considering the histogram of diagonal lines length, written as\n\nH_D(ell)=sum_ij=1^K(1-mathbfR_i-1j-1)(1-mathbfR_i+l j+l)prod_k=0^ell - 1 mathbfR_i+k j+k\n\nDET is defined as\n\nDET = fracsum_ell = d_min^K ell H_D(ell)sum_ij = 1^K mathbfR_ij\n\nwhere the parameter d_min sets the minimum length of the diagonal lines, being usual to use d_min = 2. DET is one of the most used in the search for deterministic behavior, since diagonal lines imply that entire sequences of states mathbfx_i mathbfx_i+1  mathbfx_i+ell -1 and mathbfx_j mathbfx_j+1  mathbfx_j+ell -1 are recurrent to each other, not only single points. This translates to the system repeating a pattern for long periods, since ell points recurring means similar system behavior for ell time steps. The nonlinear quantifier DET can also detect transitions between chaotic to regular behaviour, and vice-versa. This property can be used to search for bifurcations induced by some parameter drift in a time series, for which the DET will display an increase in value.\n\nLAM is analogous to DET, but it is defined for vertical/horizontal lines, which completely changes its interpretation. It is defined as\n\nLAM = fracsum_ell = v_min^K ell H_V(ell)sum_ij = 1^K mathbfR_ij\n\nwith\n\nH_V(ell)=sum_ij=1^K(1-mathbfR_ij-1)(1-mathbfR_i j+l)prod_k=0^ell - 1 mathbfR_i j+k\n\nand the parameter v_min being similar to d_min. This RQA captures information about laminar behaviour. It has this interpretation because for a straight line of length ell to form in the RP, a single state mathbfx_i has to recur with ell other different states mathbfx_j  mathbfx_j+ell-1. As a consequence, the system has to stay close to a single state for ell time steps, i.e., the longer the line, the more laminar the system is. It has been shown that LAM is also able to capture dynamical transitions, much like DET, but usually even between different chaotic regimes. Henceforth, this is also a very relevant nonlinear measure extracted from RPs.","category":"section"},{"location":"theory/#Recurrence-Microstates-Analysis","page":"Theoretical Overview","title":"Recurrence Microstates Analysis","text":"The Recurrence Microstates Analysis (RMA) has been introduced in 2018 (read the paper here)[4], defining a new concept of microstates for a RP that is associated with features of the dynamics of the time series. This concept was used to define the quantifier of recurrence entropy (RME), written as\n\nRME = - sum_I=0^N p_I^(k) ln p_I^(k)\n\nwhere I is the microstate index, k is its size, N the total amount of microstates that can be exist, being N = 2^k^2 for square motifs and p_I^(k) the probability to found a specific microstate in a system, given by its distribution. In the following years it was shown that these structures and their distributions could be used in other applications, such as characterizationg of random dynamical systems (Yoshito Hirata, 2021) [5], training neural networks (G. S. Spezzatto et al., 2024) [6], new quantifier, such as involution simmetry (G. Marghoti et al., 2024) [7] or disorder (not published yet), and estimation of RQA density-based measures (F. E. L. da Cruz et al., 2025) [8].\n\nThe goal of the RecurrenceMicrostatesAnalysis.jl library is to make this research field fast and easy to use, serving as a powerful tool for advancing new research and practical applications, such as in machine learning ðŸ˜‰.\n\nWe will explain better how compute motifs without explictly construct a RP in motifs: shapes and sampling. About how estimate RQA using RMA, we use the expression proposed in [8]:\n\nDET approx 1 - fracmathbfd^(1)cdotmathcalR^(3)cdot mathbfp^(3)RR\n\nLAM approx 1 - fracmathbfv^(1)cdotmathcalR^(3)cdot mathbfp^(3)RR\n\nThe vectors mathbfd and mathbfv are selecting the necessary terms from the resulting vector mathcalR^(3)cdot mathbfp^(3). In practice, for DET, the probability of all 64 motifs of the form\n\nbeginpmatrix 0  xi  xi  xi  1  xi  xi  xi  0  endpmatrix\n\nare summed, for all possible combinations of xi. This works for the minimal length of 2, which is the implemented case. Similarly, for LAM, it uses motifs of the form\n\nbeginpmatrix 0  1  0  xi  xi  xi  xi  xi  xi  endpmatrix \n\nWe optimize this process defining two new motifs shapes: diagonal and line. These shapes are derived from the matrices above, where it is possible to disregard the combination of xi without loss of generality, and use diagonal and lines structures to construct a probability distribution. With this, all motif of the forms presented above can be identified by the diagonal or line motif represented by I = 2.","category":"section"},{"location":"recurrence/#Recurrence-functions","page":"Recurrence functions","title":"Recurrence functions","text":"Given a trajectory mathbfx_t in mathbbR^m, with t in 1 2  K and K being the length of the analyzed time series, we say that two points i and j from that trajectory are or not recurrent based on a recurrence function. Originally, the recurrence function used for RPs are:\n\nmathbfR_ij = Theta(varepsilon - mathbfx_i - mathbfx_j)\n\nwhere Theta is the Heaviside function, cdot denotes an appropriate norm, and varepsilon is  a threshold parameter that defines the maximum distance between two points for them to be considered varepsilon-recurrent to each other. However, there are several variations that can be used as recurrence functions, as compiled in [1], and RecurrenceMicrostatesAnalysis.jl aims to provide support for their implementation.\n\nThe library include two recurrence functions: the standard, presented above, and a recurrence function with corridor threshold, proposed in [12]. The standard version code is\n\n@inline function recurrence(x::Matrix{Float64}, y::Matrix{Float64}, threshold::Float64, idx::AbstractVector{Int}, metric, _)\n    return @inbounds evaluate(metric, view(x, :, idx[1]), view(y, :, idx[2])) <= threshold\nend\n\nThe metric is computed using the Distances.jl library. Here, you can see that the function receives six arguments: the datasets x and y, the threshold, the iterator idx (which is computed by the shape and sampling functions), a metric from Distances.jl, and an ignored parameter that we will discuss later.\n\nThe structure of a recurrence function is consistent, so the recurrence function with a corridor threshold â€” given by the equation below\n\nmathbfR_ij=Theta(mathbfx_i-mathbfx_j-varepsilon_min)cdotTheta(varepsilon_max -mathbfx_i-mathbfx_j) \n\nwas implemented as follows in Julia:\n\n@inline function recurrence(x::Matrix{Float64}, y::Matrix{Float64}, thresholds::Tuple{Float64, Float64}, idx::AbstractVector{Int}, metric, _)\n    distance = @inbounds evaluate(metric, view(x, :, idx[1]), view(y, :, idx[2]))\n    return (distance > thresholds[1] && distance <= thresholds[2])\nend\n\nNote that the only difference between them is the thresholds parameter, which is a Tuple{Float64, Float64} instead of a single Float64. This is because it is a free parameter that can be adapted to your specific use case. So, if you implement a different recurrence function, this approach allows you to easily pass constant parameters.\n\nIt is also possible to adapt this for the spatial generalization of recurrence plots, as proposed in [9]. For this purpose, we use the previously ignored parameter, which represents the number of dimensions of x and y â€” more precisely, [dims(x) - 1, dims(y) - 1]. This allows us to access the iterator correctly and to unpack each component vector from a spatial problem using a view:\n\n@inline function recurrence(x::AbstractArray, y::AbstractArray, threshold::Float64, idx::AbstractVector{Int}, metric, dim::AbstractVector{Int})\n    return @inbounds evaluate(metric, view(x, :, view(idx, 1:dim[1])), view(y, :, view(idx, dim[1]+1:dim[1] + dim[2]))) <= threshold\nend\n\n@inline function recurrence(x::AbstractArray, y::AbstractArray, thresholds::Tuple{Float64, Float64}, idx::AbstractVector{Int}, metric, dim::AbstractVector{Int})\n    distance = @inbounds evaluate(metric, view(x, :, view(idx, 1:dim[1])), view(y, :, view(idx, dim[1]+1:dim[1] + dim[2])))\n    return (distance > thresholds[1] && distance <= thresholds[2])\nend","category":"section"},{"location":"quickstart/#Recurrence-Motifs-Distributions-in-One-Minute","page":"Quick Start","title":"Recurrence Motifs Distributions in One Minute","text":"The following code is an example of how to use RecurrenceMicrostatesAnalysis.jl to compute the motif distribution of a uniform distribution. Try pasting it into the REPL prompt ðŸ˜‰.\n\n##  Install everything that we need\nusing Pkg; Pkg.add(\"Distributions\"); Pkg.add(\"RecurrenceMicrostatesAnalysis\")\nusing Distributions                             #   For generate our uniform distribution\nusing RecurrenceMicrostatesAnalysis             #   !! Import RecurrenceMicrostatesAnalysis.jl\n\n##  Generate our data\ndata = rand(Uniform(0, 1), 1000)\n\n##  Square motif side\nn = 3\n\n##  Compute the threshold that maximize the recurrence entropy.\nth, s = find_parameters(data, n)\n\n##  Compute the recurrence motif probabilities distribution.\ndist = distribution(data, th, n)\n\nThe output will be a set of 512 probabilities. The distribution function samples 5 of all motifs available in the RP, regardless of overlap or repetition. Each value represents the probability of encountering a motif with a decimal representation I within the RP.\n\nIt is not necessary to compute the Recurrence Plot (RP), as the library calculates the recurrences internally without needing to construct one explicitly.\n\nnote: Note\nIt is important to remember that Julia's indexing starts at 1 instead of 0. Therefore, in the library, we define I = i - 1, where i is the Julia's index.","category":"section"},{"location":"quickstart/#Easy-RQA-Estimation","page":"Quick Start","title":"Easy RQA Estimation","text":"One of the main goals of RecurrenceMicrostatesAnalysis.jl is to provide high performance when estimating typical RQA quantifiers, such as determinism (DET) and laminarity (LAM). This process is very simple, as you can see in the following code, give it a try ðŸ˜.\n\nentropy = rentropy(dist)        #   Recurrence entropy\nrr = rrate(dist)                #   Recurrence rate\ndet = determinism(rr, dist)     #   Determinism\nlam = laminarity(rr, dist)      #   Laminarity\nÎž = disorder(data, n)           #   Disorder\n\nIt is also possible to skip computing the recurrence distribution by using an alternative overload of the determinism and laminarity functions.\n\ndet = determinism(data, th)\nlam = laminarity(data, th)","category":"section"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/#Application-with-Flux.jl","page":"Examples","title":"Application with Flux.jl","text":"Flux is a user-friendly machine learning library in Julia that provides a wide range of functionalities. In this example, we demonstrate how to use Flux.jl together with RecurrenceMicrostatesAnalysis.jl to train a multi-layer perceptron (MLP) to classify a dynamical system, based on [6].","category":"section"},{"location":"examples/#Importing-Required-Packages","page":"Examples","title":"Importing Required Packages","text":"using Flux\nusing DifferentialEquations\nusing RecurrenceMicrostatesAnalysis","category":"section"},{"location":"examples/#Generating-Data-from-the-Lorenz-System","page":"Examples","title":"Generating Data from the Lorenz System","text":"We use a Lorenz system integrated with DifferentialEquations.jl as our data source:\n\nfunction lorenz(Ïƒ, Ï, Î²; u0 = rand(3), tspan = (0.0, 5000.0))\n    function lorenz!(du, u, p, dt)\n        x, y, z = u\n        \n        du[1] = Ïƒ * (y - x)\n        du[2] = x * (Ï - z) - y\n        du[3] = x * y - Î² * z\n    end\n\n    prob = ODEProblem(lorenz!, u0, tspan)\n    sol = solve(prob, dt = 0.00001)\n\n   ##   Apply the vicinity (important!)\n   return prepare(sol, 0.2; transient = 40000, K = 500)\nend","category":"section"},{"location":"examples/#Experiment-Settings","page":"Examples","title":"Experiment Settings","text":"Ï_cls = [26.0, 26.5, 27.0, 27.5, 28.0, 28.5, 29.0, 29.5, 30.0]      # Our classes\nnum_samples_to_test = 100           # Samples to test\nnum_samples_to_train = 400          # Samples to train\n\nepoches = 80            # Epoches to train\nlearning_rate = 0.002   # Learning rate\n\nmotif_n = 3             # Motif size","category":"section"},{"location":"examples/#Defining-the-Model","page":"Examples","title":"Defining the Model","text":"model = Chain(\n    Dense(2^(motif_n * motif_n) + 2 => 512, identity),\n    Dense(512 => 256, selu),\n    Dense(256 => 64, selu),\n    Dense(64 => length(Ï_cls)),\n    softmax\n)\n\nmodel = f64(model)      # We use Float64 precision ðŸ˜„","category":"section"},{"location":"examples/#Preparing-the-Dataset","page":"Examples","title":"Preparing the Dataset","text":"Our input features include the threshold, recurrence entropy, and recurrence distribution:\n\ndata_train = zeros(Float64, 2^(motif_n * motif_n) + 2, num_samples_to_train, length(Ï_cls))\ndata_test = zeros(Float64, 2^(motif_n * motif_n) + 2, num_samples_to_test, length(Ï_cls))\n\nlabels_train = zeros(Float64, num_samples_to_train, length(Ï_cls))\nlabels_test = zeros(Float64, num_samples_to_test, length(Ï_cls))\n\nfor j in eachindex(Ï_cls)\n\n    labels_train[:, j] .= Ï_cls[j]\n    labels_test[:, j] .= Ï_cls[j]\n\n    for i in 1:num_samples_to_train\n        serie = lorenz(10.0, Ï_cls[j], 8.0/3.0)\n        th, s = find_parameters(serie, motif_n)\n        data_train[1, i, j] = th\n        data_train[end, i, j] = s\n\n        data_train[2:end-1, i, j] .= distribution(serie, th, motif_n)\n    end\n\n    for i in 1:num_samples_to_test\n        serie = lorenz(10.0, Ï_cls[j], 8.0/3.0)\n        th, s = find_parameters(serie, motif_n)\n        data_test[1, i, j] = th\n        data_test[end, i, j] = s\n\n        data_test[2:end-1, i, j] .= distribution(serie, th, motif_n)\n    end\nend\n\n##  Reshape to match the expected input format for Flux.\ndata_train = reshape(data_train, 2^(motif_n * motif_n) + 2, num_samples_to_train *  length(Ï_cls))\ndata_test = reshape(data_test, 2^(motif_n * motif_n) + 2, num_samples_to_test * length(Ï_cls))\n\nlabels_train = reshape(labels_train, num_samples_to_train * length(Ï_cls))\nlabels_test = reshape(labels_test, num_samples_to_test * length(Ï_cls))","category":"section"},{"location":"examples/#Training-the-MLP","page":"Examples","title":"Training the MLP","text":"labels_train = Flux.onehotbatch(labels_train, Ï_cls)\nlabels_test = Flux.onehotbatch(labels_test, Ï_cls)\n\nloader = Flux.DataLoader((data_train, labels_train), batchsize = 32, shuffle = true)\nopt = Flux.setup(Flux.Adam(learning_rate), model)\n\nfor epc in 1:epoches\n    for (x, y) in loader\n        _, grads = Flux.withgradient(model) do m\n            y_hat = m(x)\n            Flux.crossentropy(y_hat, y)\n        end\n\n        Flux.update!(opt, model, grads[1])\n    end\nend","category":"section"},{"location":"examples/#Evaluation","page":"Examples","title":"Evaluation","text":"We evaluate the trained model using a confusion matrix and obtain an accuracy of 91%: (Image: Confusion Matrix)","category":"section"},{"location":"#A-Julia-library-for-analyzing-dynamical-systems-with-recurrence-microstates","page":"Welcome","title":"A Julia library for analyzing dynamical systems with recurrence microstates","text":"Recurrence Microstates Analysis (RMA) is an advanced approach that generalizes the analysis of recurrence structures by capturing the statistical properties of recurrence motifs. RecurrenceMicrostatesAnalysis.jl is an efficient Julia package for performing RMA, offering support for a wide range of motif shapes, flexible sampling strategies, and comprehensive distribution computation capabilities. Furthermore, the library features an optimized pipeline for estimating standard RQA quantifiers, with significantly reduced memory and computational requirements, making it particularly well-suited for large-scale datasets.","category":"section"},{"location":"#Installation","page":"Welcome","title":"Installation","text":"Download Julia 1.8 or later, preferably the current stable release. \n\nYou can add RecurrenceMicrostatesAnalysis.jl using Julia's package manager. In the Julia prompt, you can use the following code snippets:\n\nusing Pkg\nPkg.add(\"RecurrenceMicrostatesAnalysis\")\n\nor, in Pkg REPL mode write:\n\n] add RecurrenceMicrostatesAnalysis\n\ntodo: GitHub\nRecurrenceMicrostatesAnalysis.jl is an open-source library available at GitHub repository DynamicsUFPR/RMA.jl. If you have found this library useful, please consider starring it on GitHub ðŸ˜‰.","category":"section"},{"location":"#Learning-RMA","page":"Welcome","title":"Learning RMA","text":"If you have worked with recurrence microstates analysis before, the Quick Start page offers a brief guide on how to apply the RecurrenceMicrostatesAnalysis.jl to time series data and dynamical systems.\n\nIf you haven't, then you might prefer the Theoretical Overview page, which provides a quick and simple introduction about the recurrence microstates field. The rest of the guide explains how to use the library to compute recurrence motifs probability distributions and calculate common recurrence quantifiers, along with descriptions of all available configuration options. We also include the Utils page, which covers utility functions to simplify the use of RecurrenceMicrostatesAnalysis.jl, and the Performance Tips page, where we discuss how to improve the libraryâ€™s usage performance.","category":"section"},{"location":"#Citation","page":"Welcome","title":"Citation","text":"If you use our library, please cite us ðŸ˜ƒ\n\n@article{10.1063/5.0293708,\n    author = {Vinicius Ferreira, Gabriel and Lopes da Cruz, Felipe Eduardo and Marghoti, Gabriel and de Lima Prado, Thiago and Roberto Lopes, Sergio and Marwan, Norbert and Kurths, JÃ¼rgen},\n    title = {RecurrenceMicrostatesAnalysis.jl: A Julia library for analyzing dynamical systems with recurrence microstates},\n    journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},\n    volume = {35},\n    number = {11},\n    pages = {113123},\n    year = {2025},\n    month = {11},\n    issn = {1054-1500},\n    doi = {10.1063/5.0293708},\n    url = {https://doi.org/10.1063/5.0293708}\n}","category":"section"}]
}
